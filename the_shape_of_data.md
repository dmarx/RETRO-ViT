# The Shape Of Data

![](https://img.shields.io/badge/tag-publication-lightgrey)

(originally via self-email, 2021-07-05)

Popular science book describing for normal people geometric stuff in ML (e.g. embeddings, gaussian annulus, cosine similarity, manifold hypothesis)

## Misc topic ideas

* distances and similarity
  * bag of words
  * tf-idf
  * word2vec
  * manhattan distance vs. euclidean distance vs. cosine sim
* data as manifold
* model as manifold
* embeddings and "features"
* sparsity, bottlenecks, and decompositions
  * PCA
  * fourier
* latent space
* autoencoders and denoisers
* weirdness of high dimensions
  * vector components -> rank -> dimensions -> span
  * gaussian annulus, expectations, zero-point energy, and the optimal fighter jet seat configuration
  * spherical traversals
  * sphere packing paradox
  * L1 vs L2 regularization
* GAN spaces
* CLIP space
* denoising and diffusion
* composition of representational spaces in LDM
* curse of dimensionality
* saddles, minimia, curvature, flatness
* connectedness of solutions
* power laws and 1-9-90 phenomena
* graph structures, robustness, communication
* iterative systems, chaos, attractors, cycles
* learning dynamics
* double descent
* network capacity / VC dimension
* receptive field of a neuron
* hyperbolic spaces and hierarchical representations

---

Some additional ideas brainstorming with ChatGPT:

* Receptive field: The receptive field of a neuron in a neural network is the region of the input data that the neuron is sensitive to. It determines what information the neuron is able to "see" and use to make predictions. The geometry and topology of the receptive field can affect the performance of the model.
* Attention mechanisms: Attention mechanisms are techniques used in neural networks to allow the model to "pay attention" to specific parts of the input data when making predictions. They can be thought of as "focus" mechanisms that allow the model to selectively focus on relevant information and ignore irrelevant information. The geometry and topology of the attention mechanisms can affect the interpretability and effectiveness of the model.
* Geometry of optimization: The geometry of the optimization landscape (i.e. the space of all possible model parameters) can affect the performance of optimization algorithms such as gradient descent. For example, the presence of local minima or saddle points can make it difficult for the algorithm to find a good solution. Understanding the geometry of the optimization landscape can help us design better optimization algorithms and improve the performance of machine learning models.
* Topology of solutions: The topology of the space of solutions (i.e. the space of all possible models) can affect the performance and generalization of machine learning algorithms. For example, if the space of solutions is highly connected (i.e. there are many paths between any two solutions), then it may be easier for the algorithm to find a good solution. On the other hand, if the space of solutions is highly fragmented, it may be more difficult for the algorithm to find a good solution.
* Topological data analysis: Topological data analysis (TDA) is a field that uses techniques from topology (the study of the global structure of spaces) to analyze data. TDA can be used to identify patterns and relationships in data that are not easily visible using traditional techniques. The geometry and topology of the data can affect the performance of TDA algorithms and the insights that they can provide.
* Manifold learning: Manifold learning is a set of techniques used to identify and model the underlying structure of data that lies on a low-dimensional manifold embedded in a high-dimensional space. Manifold learning algorithms can be used to visualize and analyze data, and can be particularly useful for tasks such as anomaly detection or clustering. The geometry and topology of the manifold can affect the performance of manifold learning algorithms.
* Differentiable programming: Differentiable programming is a programming paradigm that allows us to define machine learning models using ordinary programming languages and to optimize them using gradient-based optimization algorithms. Differentiable programming allows us to design machine learning models that are more flexible and expressive than traditional models, and to take advantage of the geometry and topology of the optimization landscape to improve the performance of the model.
* Geometric deep learning: Geometric deep learning is a subfield of machine learning that focuses on designing machine learning models that are able to process data with geometric or topological structure, such as graphs or manifolds. These models can be thought of as "shape-aware" models that can learn to recognize patterns in the data that are specific to the shape of the data. The geometry and topology of the data can affect the performance of geometric deep learning models.
* Geometry of generalization: The geometry of the space of solutions (i.e. the space of all possible models) can affect the generalization performance of machine learning algorithms. For example, if the space of solutions is highly connected (i.e. there are many paths between any two solutions), then it may be easier for the algorithm to generalize to new data. On the other hand, if the space of solutions is highly fragmented, it may be more difficult for the algorithm to generalize. Understanding the geometry of the space of solutions can help us design better machine learning algorithms and improve the generalization performance of models.
* Geometry of neural networks: The geometry of neural networks (i.e. the arrangement of the neurons and connections in the network) can affect the performance and generalization of the model. For example, the number and arrangement of layers in the network, the connectivity patterns between neurons, and the type of activation functions used can all affect the geometry of the model and how it processes the data. Understanding the geometry of neural networks can help us design better models and improve their performance.

