# mini RWKV port

![](https://img.shields.io/badge/tag-ggml-lightgrey)  
![](https://img.shields.io/badge/tag-experimental-lightgrey)  
![](https://img.shields.io/badge/tag-model_compression-lightgrey)  
![](https://img.shields.io/badge/tag-RNN-darkgreen)  
![](https://img.shields.io/badge/tag-tooling-lightgrey)  
![](https://img.shields.io/badge/tag-completed-darkgreen)  
![](https://img.shields.io/badge/tag-wip-lightgrey)  
![](https://img.shields.io/badge/tag-mobilenet-lightgrey)


ggml port of https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio

also doesn't *have* to be ggml, just need to try some model compression techniques and see what happens. quantization might work better or worse for RNNs.

probably a lot of lit on compressing RNNs, maybe look into mobilenet type stuff.
