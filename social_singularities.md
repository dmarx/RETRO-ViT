# Social Media and Singularity Theory

labels: alignment, publication

This whole situation with volatility in the social media adoption space is a perfect demonstration of why "singularity theory" has become a big deal in explaining deep learning recently. Before we can adopt the "best" technology, as a society, we have to abandon the old one. But this requires achieving a critical mass, after which a phase change (adapting to the new normal) can happen. This kind of phase change happens at what's called a "singularity" -- used here in the same sense as how a black hole is a singularity. It's a geometry term of art. -- On either side of the singularity is a buffer region of slow change. Inertia. As you approach a region where change is possible, you accelerate towards that change as you get closer to it, but when you're not basically right on top of it, it's hard to get enough momentum to meaningfully change. We see this sort of phenomena all over the place socially. The social equivalent is Thomas Kuhn's "paradigm shifts" that happen in the neighborhood of "scientific revolutions". The revolution is a "signularity". Back to deep learning. So one of the subtle takeaways from the whole singularity thing is that if you *want* to change, you want to ride as close as you can to that volatile region where change is possible, and apply a kind of "pressure" that encourages your current state to move towards improvement. in deep learning, this pressure is the loss function, and the singularities represent successfully modeling a facet of the problem. so by understanding the geometry of the problem space like this, we can make inferences about how to steer the gradient towards singularities to learn as quickly as possible. back to social phenomena. locking in to a technology has its pros and cons. one of the pros is that we're all communicating with each other. but one of the cons is that it's harder to innovate as a society. geometrically: the closer we get to instabilities, the easier it is for society-wide innovation to be possible. but this only works if we apply the correct "pressures" to the system. While we're in the neighborhood of an instability, we're more susceptible to pressure and change generally. imho, the "alignment" problem is less about AGI specifically and more about applying the correct "pressure" in the neighborhood of  societal instability to push it towards improvement generally, while keeping it structurally in the region where continued improvement is still possible and not "overshooting" into a new state of stuck-in-the-mud, like we have with twitter lock-in.
