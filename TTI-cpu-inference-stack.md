# TTI cpu inference stack

* port models to ONNX and compile
* quantize weights, low precision
* zero off-loading
